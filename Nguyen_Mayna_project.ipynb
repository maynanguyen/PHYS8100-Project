{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-hartford",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polar-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"6WL_DRS_Noise_Train_70.csv\") \n",
    "test = pd.read_csv(\"6WL_DRS_Noise_Test_30.csv\")\n",
    "\n",
    "# Separate into x = wavelength data, y = parameter\n",
    "# TRAIN\n",
    "train_wavelengths = train[[\"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\"]].to_numpy()\n",
    "BVF_train = train[\"BVF\"].to_numpy()\n",
    "musp_train = train[\"musp\"].to_numpy()\n",
    "B_train = train[\"B\"].to_numpy()\n",
    "Mel_train = train[\"Mel\"].to_numpy()\n",
    "O2_train = train[\"O2\"].to_numpy()\n",
    "#print(type(train_wavelengths))\n",
    "\n",
    "# TEST\n",
    "test_wavelengths = test[[\"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\"]].to_numpy()\n",
    "BVF_test = test[\"BVF\"].to_numpy()\n",
    "musp_test = test[\"musp\"].to_numpy()\n",
    "B_test = test[\"B\"].to_numpy()\n",
    "Mel_test = test[\"Mel\"].to_numpy()\n",
    "O2_test = test[\"O2\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outdoor-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(actual, pred):\n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "def MAE(actual,pred):\n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs(actual - pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-entry",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "surprised-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "commercial-grave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 2.757457998833921\n",
      "Mean absolute error: 0.09186466929289644\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "BVFregr = RandomForestRegressor(n_estimators=50, max_depth=20, min_samples_split=2, random_state=0)\n",
    "BVFregr.fit(X, BVF_train)\n",
    "\n",
    "# run\n",
    "BVF_pred = BVFregr.predict(test_wavelengths)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "coordinated-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 4.979418443956456\n",
      "Mean absolute error: 0.9700022590161562\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "musp_regr = RandomForestRegressor(n_estimators=50, max_depth=20, min_samples_split=2, random_state=0)\n",
    "musp_regr.fit(X, musp_train)\n",
    "\n",
    "# run\n",
    "musp_pred = musp_regr.predict(test_wavelengths)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "corrected-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 8.752968152111835\n",
      "Mean absolute error: 0.15839366233411212\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "Bregr = RandomForestRegressor(n_estimators=50, max_depth=20, min_samples_split=2, random_state=0)\n",
    "Bregr.fit(X, B_train)\n",
    "\n",
    "# run\n",
    "B_pred = Bregr.predict(test_wavelengths)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "objective-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 4.829866042875651\n",
      "Mean absolute error: 0.04367749635524767\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "Melregr = RandomForestRegressor(n_estimators=50, max_depth=20, min_samples_split=2, random_state=0)\n",
    "Melregr.fit(X, Mel_train)\n",
    "\n",
    "# run\n",
    "Mel_pred = Melregr.predict(test_wavelengths)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "together-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 8.36843465741302\n",
      "Mean absolute error: 6.396273586803754\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "O2regr = RandomForestRegressor(n_estimators=50, max_depth=20, min_samples_split=2, random_state=0)\n",
    "O2regr.fit(X, O2_train)\n",
    "\n",
    "# run\n",
    "O2_pred = O2regr.predict(test_wavelengths)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store RF variables\n",
    "RF_MAPE_BVF = MAPE_BVF\n",
    "RF_MAE_BVF = MAE_BVF\n",
    "RF_MAPE_musp = MAPE_musp\n",
    "RF_MAE_musp = MAE_musp\n",
    "RF_MAPE_B = MAPE_B\n",
    "RF_MAE_B = MAE_B\n",
    "RF_MAPE_Mel = MAPE_Mel\n",
    "RF_MAE_Mel = MAE_Mel\n",
    "RF_MAPE_O2 = MAPE_O2\n",
    "RF_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-hardwood",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "international-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "colonial-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 8.798187655849686\n",
      "Mean absolute error: 0.23073591124567153\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "BVFregr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, max_depth=3, random_state=0)\n",
    "BVFregr.fit(X, BVF_train)\n",
    "\n",
    "# rum\n",
    "BVF_pred = BVFregr.predict(test_wavelengths)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "trying-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 13.747719273293956\n",
      "Mean absolute error: 2.5403007233657524\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "musp_regr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, max_depth=3, random_state=0)\n",
    "musp_regr.fit(X, musp_train)\n",
    "\n",
    "# rum\n",
    "musp_pred = musp_regr.predict(test_wavelengths)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "least-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 13.937186151233094\n",
      "Mean absolute error: 0.2503895011999261\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "Bregr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, max_depth=3, random_state=0)\n",
    "Bregr.fit(X, B_train)\n",
    "\n",
    "# run\n",
    "B_pred = Bregr.predict(test_wavelengths)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "remarkable-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 14.360070007917905\n",
      "Mean absolute error: 0.11852515346540773\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "Melregr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, max_depth=3, random_state=0)\n",
    "Melregr.fit(X, Mel_train)\n",
    "\n",
    "# run\n",
    "Mel_pred = Melregr.predict(test_wavelengths)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "physical-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 12.978375810684087\n",
      "Mean absolute error: 9.967800064512389\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# fit model\n",
    "X = train_wavelengths # same training wavelengths for all models\n",
    "O2regr = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, max_depth=3, random_state=0)\n",
    "O2regr.fit(X, O2_train)\n",
    "\n",
    "# run\n",
    "O2_pred = O2regr.predict(test_wavelengths)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store GBM variables\n",
    "GBM_MAPE_BVF = MAPE_BVF\n",
    "GBM_MAE_BVF = MAE_BVF\n",
    "GBM_MAPE_musp = MAPE_musp\n",
    "GBM_MAE_musp = MAE_musp\n",
    "GBM_MAPE_B = MAPE_B\n",
    "GBM_MAE_B = MAE_B\n",
    "GBM_MAPE_Mel = MAPE_Mel\n",
    "GBM_MAE_Mel = MAE_Mel\n",
    "GBM_MAPE_O2 = MAPE_O2\n",
    "GBM_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-interface",
   "metadata": {},
   "source": [
    "# Generalized Linear Model (GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "southern-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "quiet-marble",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 25.36024815584326\n",
      "Mean absolute error: 0.5347176359116139\n"
     ]
    }
   ],
   "source": [
    "X = train_wavelengths\n",
    "BVFregr = TweedieRegressor(power=0, alpha=0.5, link='log')\n",
    "BVFregr.fit(X, BVF_train)\n",
    "\n",
    "# run\n",
    "BVF_pred = BVFregr.predict(test_wavelengths)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "appreciated-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 18.098119578169445\n",
      "Mean absolute error: 3.2384660569497945\n"
     ]
    }
   ],
   "source": [
    "X = train_wavelengths\n",
    "musp_regr = TweedieRegressor(power=0, alpha=0.1, link='log')\n",
    "musp_regr.fit(X, musp_train)\n",
    "\n",
    "# run\n",
    "musp_pred = musp_regr.predict(test_wavelengths)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "liquid-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 18.330494697454995\n",
      "Mean absolute error: 0.3274324350279938\n"
     ]
    }
   ],
   "source": [
    "X = train_wavelengths\n",
    "Bregr = TweedieRegressor(power=0, alpha=0.5, link='log')\n",
    "Bregr.fit(X, B_train)\n",
    "\n",
    "# run\n",
    "B_pred = Bregr.predict(test_wavelengths)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dangerous-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 39.724668373783096\n",
      "Mean absolute error: 0.28131408782076744\n"
     ]
    }
   ],
   "source": [
    "X = train_wavelengths\n",
    "Melregr = TweedieRegressor(power=0, alpha=0.5, link='log')\n",
    "Melregr.fit(X, Mel_train)\n",
    "\n",
    "# run\n",
    "Mel_pred = Melregr.predict(test_wavelengths)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "headed-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 9.636429823557325\n",
      "Mean absolute error: 7.443410614965039\n"
     ]
    }
   ],
   "source": [
    "X = train_wavelengths\n",
    "O2regr = TweedieRegressor(power=0, alpha=0.5, link='log')\n",
    "O2regr.fit(X, O2_train)\n",
    "\n",
    "# run\n",
    "O2_pred = O2regr.predict(test_wavelengths)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store GLM variables\n",
    "GLM_MAPE_BVF = MAPE_BVF\n",
    "GLM_MAE_BVF = MAE_BVF\n",
    "GLM_MAPE_musp = MAPE_musp\n",
    "GLM_MAE_musp = MAE_musp\n",
    "GLM_MAPE_B = MAPE_B\n",
    "GLM_MAE_B = MAE_B\n",
    "GLM_MAPE_Mel = MAPE_Mel\n",
    "GLM_MAE_Mel = MAE_Mel\n",
    "GLM_MAPE_O2 = MAPE_O2\n",
    "GLM_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-tooth",
   "metadata": {},
   "source": [
    "## Deep Learning (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "common-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a 2 layer network\n",
    "class Linear_2Hidden(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self,width):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(Linear_2Hidden, self).__init__()\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, D_out = [6, width, width, 1]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        # Define layer types\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        torch.nn.ReLU()       # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        torch.nn.ReLU()       # activation function\n",
    "        \n",
    "        x = self.linear3(x) # output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charming-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, x, y):\n",
    "    # run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    return y, y_predict\n",
    "\n",
    "#####\n",
    "def test(model, test_loader):\n",
    "    y_vectors = list()\n",
    "    y_predict_vectors = list()\n",
    "\n",
    "    batch_index = 0\n",
    "    for x, y in test_loader:\n",
    "        y, y_predict = test_batch(model=model, x=x, y=y)\n",
    "\n",
    "        y_vectors.append(y.data.numpy())\n",
    "        y_predict_vectors.append(y_predict.data.numpy())\n",
    "\n",
    "        batch_index += 1\n",
    "\n",
    "    y_predict_vector = np.concatenate(y_predict_vectors)\n",
    "\n",
    "    return y_predict_vector \n",
    "##### Train on a specific batch of data.\n",
    "def train_batch(model, x, y, optimizer, loss_fn):\n",
    "    # Run forward calculation\n",
    "    y_predict = model.forward(x)\n",
    "\n",
    "    # Compute loss.\n",
    "    y_predict = y_predict.squeeze(1)\n",
    "    loss = loss_fn(y_predict, y)\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data.item()\n",
    "\n",
    "##### Iterate over epochs and all batches of data\n",
    "def train(model, train_loader , test_loader , optimizer, loss_fn, epochs=5):\n",
    "    losses = list()\n",
    "    losses_test = list()\n",
    "    \n",
    "\n",
    "    batch_index = 0\n",
    "    for e in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            loss = train_batch(model=model, x=x, y=y, optimizer=optimizer, loss_fn=loss_fn)\n",
    "            losses.append(loss)\n",
    "\n",
    "            batch_index += 1\n",
    "            \n",
    "        for x , y in test_loader:\n",
    "            y_predict = model.forward(x)\n",
    "            losst = loss_fn(y_predict, y)\n",
    "            \n",
    "            losses_test.append(losst.data.item())\n",
    "\n",
    "#         print(\"Epoch: \", e+1)\n",
    "#         print(\"Batches: \", batch_index)\n",
    "\n",
    "    return losses , losses_test\n",
    "\n",
    "def plot_loss(losses_train,loss_test, show=True):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8,6)\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    x_loss = list(range(len(losses)))\n",
    "    plt.plot(x_loss, losses_train)\n",
    "    \n",
    "\n",
    "    a = list(range(len(loss_test)))\n",
    "    const = round(len(losses) / len(loss_test))\n",
    "    b = [(i+1)*const for i in a]\n",
    "    \n",
    "    plt.plot(b,loss_test)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "def run(data_loader_train, data_loader_test , model_to_test , width , weight_decay):\n",
    "    \n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    model = model_to_test(width=width)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate , weight_decay = weight_decay)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss , loss_test = train(model=model, train_loader=data_loader_train , test_loader = data_loader_test, \n",
    "                 optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=model, test_loader=data_loader_test)\n",
    "\n",
    "    return loss , loss_test , y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlikely-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x_dtype = torch.FloatTensor\n",
    "        y_dtype = torch.FloatTensor     # Use float for continuous data\n",
    "\n",
    "        self.length = x.shape[0]\n",
    "\n",
    "        # 1) Extract data from numpy, 2) put it in a tensor, 3) properly data type it.\n",
    "        self.x_data = torch.from_numpy(x).type(x_dtype)\n",
    "        self.y_data = torch.from_numpy(y).type(y_dtype)\n",
    "\n",
    "    # Method for getting an element.\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # Method for getting the length of the dataset\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protecting-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BVF ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = BVF_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = BVF_test)\n",
    "\n",
    "# print(\"Train set size: \", dataset_train.length)\n",
    "# print(\"Test set size: \", dataset_test.length)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "meaningful-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mayna\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([30000])) that is different to the input size (torch.Size([30000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 24.998028032306667\n",
      "Mean absolute error: 0.547850780984852\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "BVF_pred = y_predict\n",
    "BVF_pred = np.reshape(BVF_pred, 30000)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hundred-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "### musp ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = musp_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = musp_test)\n",
    "\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protected-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 19.20854464503055\n",
      "Mean absolute error: 3.341511692750001\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "musp_pred = y_predict\n",
    "musp_pred = np.reshape(musp_pred, 30000)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subtle-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "### B ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = B_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = B_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numerous-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 16.88493507743787\n",
      "Mean absolute error: 0.295159564957606\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "B_pred = y_predict\n",
    "B_pred = np.reshape(B_pred, 30000)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "human-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mel ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = Mel_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = Mel_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "usual-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 29.692027421861006\n",
      "Mean absolute error: 0.24098214403008736\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "Mel_pred = y_predict\n",
    "Mel_pred = np.reshape(Mel_pred, 30000)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rocky-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "### O2 ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = O2_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = O2_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "driven-belize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 9.385084688705486\n",
      "Mean absolute error: 7.262178227614339\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "O2_pred = y_predict\n",
    "O2_pred = np.reshape(O2_pred, 30000)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nonprofit-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DL variables\n",
    "DL_MAPE_BVF = MAPE_BVF\n",
    "DL_MAE_BVF = MAE_BVF\n",
    "DL_MAPE_musp = MAPE_musp\n",
    "DL_MAE_musp = MAE_musp\n",
    "DL_MAPE_B = MAPE_B\n",
    "DL_MAE_B = MAE_B\n",
    "DL_MAPE_Mel = MAPE_Mel\n",
    "DL_MAE_Mel = MAE_Mel\n",
    "DL_MAPE_O2 = MAPE_O2\n",
    "DL_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-medicaid",
   "metadata": {},
   "source": [
    "# Plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "supported-exception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['BVF', 'musp', 'B', 'Mel', 'O2']\n",
    "DL = [DL_MAE_BVF,DL_MAE_musp,DL_MAE_B,DL_MAE_Mel,DL_MAE_O2]\n",
    "RF = [RF_MAE_BVF,RF_MAE_musp,RF_MAE_B,RF_MAE_Mel,RF_MAE_O2]\n",
    "GBM = [GBM_MAE_BVF,GBM_MAE_musp,GBM_MAE_B,GBM_MAE_Mel,GBM_MAE_O2]\n",
    "GLM = [GLM_MAE_BVF,GLM_MAE_musp,GLM_MAE_B,GLM_MAE_Mel,GLM_MAE_O2]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, DL, width/2, label='DL')\n",
    "rects2 = ax.bar(x - width/2, RF, width/2, label='RF')\n",
    "rects3 = ax.bar(x , GBM, width/2, label='GBM')\n",
    "rects4 = ax.bar(x + width/2, GLM, width/2, label='GLM')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Mean absolute error')\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-reform",
   "metadata": {},
   "source": [
    "# Optimize Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-policy",
   "metadata": {},
   "source": [
    "Things to try: different activation function, different optimizer, width, 3 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-median",
   "metadata": {},
   "source": [
    "### Activation: tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "included-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a 2 layer network\n",
    "class Linear_2Hidden(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self,width):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(Linear_2Hidden, self).__init__()\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, D_out = [6, width, width, 1]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        # Define layer types\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "\n",
    "        x = self.linear3(x) # output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "technological-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 3.7061432031496166\n",
      "Mean absolute error: 0.1064343034960747\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = BVF_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = BVF_test)\n",
    "\n",
    "# print(\"Train set size: \", dataset_train.length)\n",
    "# print(\"Test set size: \", dataset_test.length)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "BVF_pred = y_predict\n",
    "BVF_pred = np.reshape(BVF_pred, 30000)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "monetary-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 5.9607987631024875\n",
      "Mean absolute error: 1.1034572453063967\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = musp_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = musp_test)\n",
    "\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "musp_pred = y_predict\n",
    "musp_pred = np.reshape(musp_pred, 30000)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "parallel-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 7.521700352357059\n",
      "Mean absolute error: 0.14351435375454583\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = B_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = B_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "B_pred = y_predict\n",
    "B_pred = np.reshape(B_pred, 30000)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "destroyed-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 9.987058828211394\n",
      "Mean absolute error: 0.06780838378845151\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = Mel_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = Mel_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "Mel_pred = y_predict\n",
    "Mel_pred = np.reshape(Mel_pred, 30000)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "binary-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 4.574806171346558\n",
      "Mean absolute error: 3.3887366154917404\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = O2_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = O2_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "O2_pred = y_predict\n",
    "O2_pred = np.reshape(O2_pred, 30000)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mathematical-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DL variables\n",
    "activation_MAPE_BVF = MAPE_BVF\n",
    "activation_MAE_BVF = MAE_BVF\n",
    "activation_MAPE_musp = MAPE_musp\n",
    "activation_MAE_musp = MAE_musp\n",
    "activation_MAPE_B = MAPE_B\n",
    "activation_MAE_B = MAE_B\n",
    "activation_MAPE_Mel = MAPE_Mel\n",
    "activation_MAE_Mel = MAE_Mel\n",
    "activation_MAPE_O2 = MAPE_O2\n",
    "activation_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-italy",
   "metadata": {},
   "source": [
    "### Optimizer = SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "killing-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct a 2 layer network\n",
    "class Linear_2Hidden(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self,width):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(Linear_2Hidden, self).__init__()\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, D_out = [6, width, width, 1]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        # Define layer types\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear3(x) # output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "banner-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_loader_train, data_loader_test , model_to_test , width , weight_decay):\n",
    "    \n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    model = model_to_test(width=width)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate , weight_decay = weight_decay)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss , loss_test = train(model=model, train_loader=data_loader_train , test_loader = data_loader_test, \n",
    "                 optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=model, test_loader=data_loader_test)\n",
    "\n",
    "    return loss , loss_test , y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "italic-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 9.21504712332964\n",
      "Mean absolute error: 0.2402497406654946\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = BVF_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = BVF_test)\n",
    "\n",
    "# print(\"Train set size: \", dataset_train.length)\n",
    "# print(\"Test set size: \", dataset_test.length)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "BVF_pred = y_predict\n",
    "BVF_pred = np.reshape(BVF_pred, 30000)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strong-stylus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 12.40938278010279\n",
      "Mean absolute error: 2.582665931578318\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = musp_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = musp_test)\n",
    "\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "musp_pred = y_predict\n",
    "musp_pred = np.reshape(musp_pred, 30000)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "atmospheric-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 16.09740246135767\n",
      "Mean absolute error: 0.2919928414457639\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = B_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = B_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "B_pred = y_predict\n",
    "B_pred = np.reshape(B_pred, 30000)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "challenging-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 16.770000516834457\n",
      "Mean absolute error: 0.14071694263129114\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = Mel_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = Mel_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "Mel_pred = y_predict\n",
    "Mel_pred = np.reshape(Mel_pred, 30000)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "latter-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 6.212571207518987\n",
      "Mean absolute error: 4.786939254547119\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = O2_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = O2_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "O2_pred = y_predict\n",
    "O2_pred = np.reshape(O2_pred, 30000)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "obvious-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DL variables\n",
    "optim_MAPE_BVF = MAPE_BVF\n",
    "optim_MAE_BVF = MAE_BVF\n",
    "optim_MAPE_musp = MAPE_musp\n",
    "optim_MAE_musp = MAE_musp\n",
    "optim_MAPE_B = MAPE_B\n",
    "optim_MAE_B = MAE_B\n",
    "optim_MAPE_Mel = MAPE_Mel\n",
    "optim_MAE_Mel = MAE_Mel\n",
    "optim_MAPE_O2 = MAPE_O2\n",
    "optim_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-worry",
   "metadata": {},
   "source": [
    "### Loss function = L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "golden-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_loader_train, data_loader_test , model_to_test , width , weight_decay):\n",
    "    \n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    model = model_to_test(width=width)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate , weight_decay = weight_decay)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.L1Loss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss , loss_test = train(model=model, train_loader=data_loader_train , test_loader = data_loader_test, \n",
    "                 optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=model, test_loader=data_loader_test)\n",
    "\n",
    "    return loss , loss_test , y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "reflected-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a 2 layer network\n",
    "class Linear_2Hidden(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self,width):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(Linear_2Hidden, self).__init__()\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, D_out = [6, width, width, 1]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        # Define layer types\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear3(x) # output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "modern-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mayna\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:94: UserWarning: Using a target size (torch.Size([30000])) that is different to the input size (torch.Size([30000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 5.010424494943713\n",
      "Mean absolute error: 0.1385726077918402\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = BVF_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = BVF_test)\n",
    "\n",
    "# print(\"Train set size: \", dataset_train.length)\n",
    "# print(\"Test set size: \", dataset_test.length)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "BVF_pred = y_predict\n",
    "BVF_pred = np.reshape(BVF_pred, 30000)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "polish-manhattan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 5.324981684329296\n",
      "Mean absolute error: 1.0118080839557648\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = musp_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = musp_test)\n",
    "\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "musp_pred = y_predict\n",
    "musp_pred = np.reshape(musp_pred, 30000)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "offshore-fundamental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 7.102751896551923\n",
      "Mean absolute error: 0.13124454184384662\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = B_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = B_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "B_pred = y_predict\n",
    "B_pred = np.reshape(B_pred, 30000)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adopted-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 8.127907520215155\n",
      "Mean absolute error: 0.07007454558843665\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = Mel_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = Mel_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "Mel_pred = y_predict\n",
    "Mel_pred = np.reshape(Mel_pred, 30000)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "turned-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 3.76161292650809\n",
      "Mean absolute error: 2.8805162584045414\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = O2_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = O2_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_2Hidden, width = 200 , weight_decay = 0)\n",
    "O2_pred = y_predict\n",
    "O2_pred = np.reshape(O2_pred, 30000)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beneficial-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DL variables\n",
    "loss_MAPE_BVF = MAPE_BVF\n",
    "loss_MAE_BVF = MAE_BVF\n",
    "loss_MAPE_musp = MAPE_musp\n",
    "loss_MAE_musp = MAE_musp\n",
    "loss_MAPE_B = MAPE_B\n",
    "loss_MAE_B = MAE_B\n",
    "loss_MAPE_Mel = MAPE_Mel\n",
    "loss_MAE_Mel = MAE_Mel\n",
    "loss_MAPE_O2 = MAPE_O2\n",
    "loss_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-movement",
   "metadata": {},
   "source": [
    "### 3 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "medieval-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct a 3 layer network\n",
    "class Linear_3Hidden(nn.Module):\n",
    "    '''\n",
    "    A simple, general purpose, fully connected network\n",
    "    '''\n",
    "    def __init__(self,width):\n",
    "        # Perform initialization of the pytorch superclass\n",
    "        super(Linear_3Hidden, self).__init__()\n",
    "        \n",
    "        # Define network layer dimensions\n",
    "        D_in, H1, H2, H3, D_out = [6, width, width, width, 1]    # These numbers correspond to each layer: [input, hidden_1, output]\n",
    "        \n",
    "        # Define layer types\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H2, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This method defines the network layering and activation functions\n",
    "        '''\n",
    "        x = self.linear1(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear2(x) # hidden layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear3(x) # output layer\n",
    "        x = torch.tanh(x)       # activation function\n",
    "        \n",
    "        x = self.linear4(x) # output layer\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deluxe-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_loader_train, data_loader_test , model_to_test , width , weight_decay):\n",
    "    \n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    learning_rate = 1e-3\n",
    "    model = model_to_test(width=width)\n",
    "    \n",
    "    # Initialize the optimizer with above parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate , weight_decay = weight_decay)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.MSELoss()  # mean squared error\n",
    "\n",
    "    # Train and get the resulting loss per iteration\n",
    "    loss , loss_test = train(model=model, train_loader=data_loader_train , test_loader = data_loader_test, \n",
    "                 optimizer=optimizer, loss_fn=loss_fn)\n",
    "    \n",
    "    # Test and get the resulting predicted y values\n",
    "    y_predict = test(model=model, test_loader=data_loader_test)\n",
    "\n",
    "    return loss , loss_test , y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "smooth-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 4.308949763645162\n",
      "Mean absolute error: 0.11792087941714284\n"
     ]
    }
   ],
   "source": [
    "### BVF ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = BVF_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = BVF_test)\n",
    "\n",
    "# print(\"Train set size: \", dataset_train.length)\n",
    "# print(\"Test set size: \", dataset_test.length)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_3Hidden, width = 200 , weight_decay = 0)\n",
    "BVF_pred = y_predict\n",
    "BVF_pred = np.reshape(BVF_pred, 30000)\n",
    "MAPE_BVF = MAPE(BVF_test, BVF_pred)\n",
    "MAE_BVF = MAE(BVF_test, BVF_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_BVF)\n",
    "print(\"Mean absolute error:\", MAE_BVF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "leading-sally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 5.081618116188605\n",
      "Mean absolute error: 0.9366752084828694\n"
     ]
    }
   ],
   "source": [
    "### musp ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = musp_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = musp_test)\n",
    "\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_3Hidden, width = 200 , weight_decay = 0)\n",
    "musp_pred = y_predict\n",
    "musp_pred = np.reshape(musp_pred, 30000)\n",
    "MAPE_musp = MAPE(musp_test, musp_pred)\n",
    "MAE_musp = MAE(musp_test, musp_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_musp)\n",
    "print(\"Mean absolute error:\", MAE_musp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "timely-failing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 7.55008802879265\n",
      "Mean absolute error: 0.13993468875493048\n"
     ]
    }
   ],
   "source": [
    "### B ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = B_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = B_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_3Hidden, width = 200 , weight_decay = 0)\n",
    "B_pred = y_predict\n",
    "B_pred = np.reshape(B_pred, 30000)\n",
    "MAPE_B = MAPE(B_test, B_pred)\n",
    "MAE_B = MAE(B_test, B_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_B)\n",
    "print(\"Mean absolute error:\", MAE_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "through-athens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 6.907444885615206\n",
      "Mean absolute error: 0.057981358202310956\n"
     ]
    }
   ],
   "source": [
    "### Mel ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = Mel_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = Mel_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_3Hidden, width = 200 , weight_decay = 0)\n",
    "Mel_pred = y_predict\n",
    "Mel_pred = np.reshape(Mel_pred, 30000)\n",
    "Mel_pred_new = np.delete(Mel_pred, np.where(Mel_test == 0))\n",
    "Mel_test_new = np.delete(Mel_test, np.where(Mel_test == 0))\n",
    "MAPE_Mel = MAPE(Mel_test_new, Mel_pred_new)\n",
    "MAE_Mel = MAE(Mel_test, Mel_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_Mel)\n",
    "print(\"Mean absolute error:\", MAE_Mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "first-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute percent error: 14.631384537566305\n",
      "Mean absolute error: 11.126042918103025\n"
     ]
    }
   ],
   "source": [
    "### O2 ###\n",
    "# Recast the training and test data sets into proper pytorch tensors.\n",
    "\n",
    "dataset_train = dataset(x = train_wavelengths, y = O2_train)\n",
    "dataset_test = dataset(x = test_wavelengths, y = O2_test)\n",
    "\n",
    "# Batch size is the number of training examples used to calculate each iteration's gradient\n",
    "batch_size_train = 16\n",
    "\n",
    "# Construct data loader iterables for the training and test data sets.\n",
    "data_loader_train = DataLoader(dataset=dataset_train,batch_size=batch_size_train, shuffle=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=len(dataset_test), shuffle=False) # No need to batch\n",
    "\n",
    "# run\n",
    "losses, loss_test, y_predict = run(data_loader_train=data_loader_train, data_loader_test=data_loader_test ,\n",
    "                        model_to_test = Linear_3Hidden, width = 200 , weight_decay = 0)\n",
    "O2_pred = y_predict\n",
    "O2_pred = np.reshape(O2_pred, 30000)\n",
    "MAPE_O2 = MAPE(O2_test, O2_pred)\n",
    "MAE_O2 = MAE(O2_test, O2_pred)\n",
    "print(\"Mean absolute percent error:\", MAPE_O2)\n",
    "print(\"Mean absolute error:\", MAE_O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "focused-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DL variables\n",
    "threelayer_MAPE_BVF = MAPE_BVF\n",
    "threelayer_MAE_BVF = MAE_BVF\n",
    "threelayer_MAPE_musp = MAPE_musp\n",
    "threelayer_MAE_musp = MAE_musp\n",
    "threelayer_MAPE_B = MAPE_B\n",
    "threelayer_MAE_B = MAE_B\n",
    "threelayer_MAPE_Mel = MAPE_Mel\n",
    "threelayer_MAE_Mel = MAE_Mel\n",
    "threelayer_MAPE_O2 = MAPE_O2\n",
    "threelayer_MAE_O2 = MAE_O2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-thought",
   "metadata": {},
   "source": [
    "# Plot errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sophisticated-suggestion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['BVF', 'musp', 'B', 'Mel', 'O2']\n",
    "DL = [DL_MAE_BVF,DL_MAE_musp,DL_MAE_B,DL_MAE_Mel,DL_MAE_O2] # original\n",
    "activation = [activation_MAE_BVF,activation_MAE_musp,activation_MAE_B,activation_MAE_Mel,activation_MAE_O2]\n",
    "optim = [optim_MAE_BVF,optim_MAE_musp,optim_MAE_B,optim_MAE_Mel,optim_MAE_O2]\n",
    "lossfn = [loss_MAE_BVF,loss_MAE_musp,loss_MAE_B,loss_MAE_Mel,loss_MAE_O2]\n",
    "threelayer = [threelayer_MAE_BVF,threelayer_MAE_musp,threelayer_MAE_B,threelayer_MAE_Mel,threelayer_MAE_O2]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, DL, width/2, label='DL')\n",
    "rects2 = ax.bar(x - width/2, activation, width/2, label='activation')\n",
    "rects3 = ax.bar(x , optim, width/2, label='optim')\n",
    "rects4 = ax.bar(x + width/2, lossfn, width/2, label='loss')\n",
    "rects4 = ax.bar(x + width, threelayer, width/2, label='3layer')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Mean absolute error')\n",
    "#ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-vermont",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
